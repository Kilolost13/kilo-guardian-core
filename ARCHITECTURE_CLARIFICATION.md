# Kilo Architecture Clarification

## âš ï¸ Important: Avoiding Resource Conflicts

You're absolutely right - running both "Kilo AI agent" and "ai-brain" on Beelink would overload it!

## ğŸ¯ Current Setup (CORRECT)

### Beelink (Lightweight Control)
```
âœ… llama.cpp         (Port 11434) - LLM inference only
âœ… k3s-manager       (Port 9011)  - Cluster management
âœ… kubectl           - CLI tool
âŒ ai-brain          - NOT running here (runs in k3s)
```

**Resource Usage on Beelink:**
- llama.cpp: ~2.7GB RAM (Phi-3-mini model loaded)
- k3s-manager: ~30MB RAM
- Total: ~2.8GB RAM

### HP K3s (Heavy Workloads)
```
âœ… kilo-ai-brain     - The actual AI agent brain
âœ… kilo-frontend     - Web UI
âœ… kilo-gateway      - API gateway
âœ… All microservices - 11 total pods
```

**Resource Usage on HP:**
- All pods combined: ~4-6GB RAM
- k3s overhead: ~1GB RAM

## ğŸ§  What is "ai-brain"?

**ai-brain IS the Kilo AI agent!** It includes:
- RAG (Retrieval-Augmented Generation)
- Memory system
- Agent orchestration
- Tool calling
- Skill execution

It runs in k3s on HP and calls back to Beelink's llama.cpp for LLM inference.

## ğŸ”„ How They Work Together

```
User Request
    â†“
kilo-frontend (HP k3s)
    â†“
kilo-gateway (HP k3s)
    â†“
kilo-ai-brain (HP k3s) â† This is the agent!
    â†“
llama.cpp (Beelink) â† Just provides LLM inference
    â†“
Response flows back
```

## âœ… What Should Run Where

### Beelink ONLY runs:
1. **llama.cpp** - Lightweight inference server
2. **k3s-manager** - Cluster management API
3. âŒ **NOT ai-brain** - Too heavy, already in k3s

### HP k3s runs:
1. **kilo-ai-brain** - The full AI agent
2. **All microservices** - Financial, habits, etc.
3. **Frontend** - Web interface

## ğŸš« Common Mistake to Avoid

**DON'T** run `services/ai_brain/main.py` directly on Beelink!
- It's already running in k3s on HP
- Running it twice would:
  - Duplicate processing
  - Compete for llama.cpp resources
  - Waste RAM on Beelink
  - Cause port conflicts

## ğŸ¯ Correct Usage

### To interact with Kilo:
1. **Via Web UI**: http://192.168.68.56:30001
2. **Via API**: http://192.168.68.56:30801
3. **Via k3s-manager**: http://localhost:9011 (from Beelink)

### To manage the cluster (from Beelink):
```bash
# Check status
curl http://localhost:9011/cluster/status

# Scale services
curl -X POST http://localhost:9011/services/scale \
  -d '{"service":"kilo-ai-brain","replicas":2}'
```

## ğŸ“Š Current Resource Footprint

**Beelink:**
- llama.cpp: 2.7GB RAM
- k3s-manager: 30MB RAM
- **Total: ~2.8GB** âœ… Safe

**HP:**
- 11 running pods: ~6GB RAM
- k3s overhead: ~1GB RAM
- **Total: ~7GB** âœ… Within capacity

## âœ¨ Summary

- âœ… **ai-brain runs ONLY in k3s on HP**
- âœ… **llama.cpp runs ONLY on Beelink**
- âœ… **They communicate via network (port 11434)**
- âœ… **Beelink stays lightweight and responsive**
- âœ… **HP handles all the heavy AI work**

This hybrid architecture keeps Beelink from "blowing up" while giving Kilo full power on the HP!
